{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import AverageMeter, accuracy\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "best_acc = 0\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint, filename=\"checkpoint.pth.tar\"):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, \"model_best.pth.tar\"))\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps,\n",
    "    num_training_steps,\n",
    "    num_cycles=7.0 / 16.0,\n",
    "    last_epoch=-1,\n",
    "):\n",
    "    def _lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        no_progress = float(current_step - num_warmup_steps) / float(\n",
    "            max(1, num_training_steps - num_warmup_steps)\n",
    "        )\n",
    "        return max(0.0, math.cos(math.pi * num_cycles * no_progress))\n",
    "\n",
    "    return LambdaLR(optimizer, _lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "def interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([-1, size] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
    "\n",
    "\n",
    "def de_interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([size, -1] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/30/2024 15:46:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "05/30/2024 15:46:38 - INFO - __main__ -   {'gpu_id': 0, 'num_workers': 4, 'dataset': 'cifar10', 'num_labeled': 4000, 'expand_labels': False, 'arch': 'wideresnet', 'total_steps': 1048576, 'eval_step': 1024, 'start_epoch': 0, 'batch_size': 8, 'lr': 0.03, 'warmup': 0.0, 'wdecay': 0.0005, 'nesterov': True, 'use_ema': True, 'ema_decay': 0.999, 'mu': 7, 'lambda_u': 1.0, 'T': 1.0, 'threshold': 0.95, 'out': 'result', 'resume': '', 'seed': None, 'amp': False, 'opt_level': 'O1', 'local_rank': -1, 'no_progress': False, 'world_size': 1, 'n_gpu': 1, 'device': device(type='cuda', index=0)}\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"gpu_id\": 0,  # ID(s) for CUDA_VISIBLE_DEVICES\n",
    "    \"num_workers\": 4,  # Number of workers\n",
    "    \"dataset\": \"cifar10\",  # Dataset name, choices are ['cifar10', 'cifar100', 'pss']\n",
    "    \"num_labeled\": 4000,  # Number of labeled data\n",
    "    \"expand_labels\": False,  # Expand labels to fit eval steps, default is not to expand\n",
    "    \"arch\": \"wideresnet\",  # Model architecture, choices are ['wideresnet', 'resnext']\n",
    "    \"total_steps\": 2**20,  # Number of total steps to run\n",
    "    \"eval_step\": 1024,  # Number of evaluation steps to run\n",
    "    \"start_epoch\": 0,  # Manual epoch number, useful on restarts\n",
    "    \"batch_size\": 8,  # Train batch size\n",
    "    \"lr\": 0.03,  # Initial learning rate\n",
    "    \"warmup\": 0.0,  # Warmup epochs, based on unlabeled data\n",
    "    \"wdecay\": 0.0005,  # Weight decay\n",
    "    \"nesterov\": True,  # Use Nesterov momentum\n",
    "    \"use_ema\": True,  # Use Exponential Moving Average model\n",
    "    \"ema_decay\": 0.999,  # EMA decay rate\n",
    "    \"mu\": 7,  # Coefficient of unlabeled batch size\n",
    "    \"lambda_u\": 1.0,  # Coefficient of unlabeled loss\n",
    "    \"T\": 1.0,  # Pseudo label temperature\n",
    "    \"threshold\": 0.95,  # Pseudo label threshold\n",
    "    \"out\": \"result\",  # Directory to output the result\n",
    "    \"resume\": \"\",  # Path to latest checkpoint, default is none\n",
    "    \"seed\": None,  # Random seed\n",
    "    \"amp\": False,  # Use 16-bit (mixed) precision through NVIDIA apex AMP\n",
    "    \"opt_level\": \"O1\",  # Apex AMP optimization level, see details at NVIDIA apex AMP documentation\n",
    "    \"local_rank\": -1,  # For distributed training: local rank\n",
    "    \"no_progress\": False,  # Don't use progress bar\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**args)\n",
    "\n",
    "if args.local_rank == -1:\n",
    "    device = torch.device(\"cuda\", args.gpu_id)\n",
    "    args.world_size = 1\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.world_size = torch.distributed.get_world_size()\n",
    "    args.n_gpu = 1\n",
    "\n",
    "args.device = device\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "\n",
    "logger.warning(\n",
    "    f\"Process rank: {args.local_rank}, \"\n",
    "    f\"device: {args.device}, \"\n",
    "    f\"n_gpu: {args.n_gpu}, \"\n",
    "    f\"distributed training: {bool(args.local_rank != -1)}, \"\n",
    "    f\"16-bits training: {args.amp}\",\n",
    ")\n",
    "\n",
    "logger.info(dict(args._get_kwargs()))\n",
    "\n",
    "if args.seed is not None:\n",
    "    set_seed(args)\n",
    "\n",
    "if args.local_rank in [-1, 0]:\n",
    "    os.makedirs(args.out, exist_ok=True)\n",
    "    args.writer = SummaryWriter(args.out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"cifar10\":\n",
    "    args.num_classes = 10\n",
    "    if args.arch == \"wideresnet\":\n",
    "        args.model_depth = 28\n",
    "        args.model_width = 2\n",
    "    elif args.arch == \"resnext\":\n",
    "        args.model_cardinality = 4\n",
    "        args.model_depth = 28\n",
    "        args.model_width = 4\n",
    "\n",
    "elif args.dataset == \"cifar100\":\n",
    "    args.num_classes = 100\n",
    "    if args.arch == \"wideresnet\":\n",
    "        args.model_depth = 28\n",
    "        args.model_width = 8\n",
    "    elif args.arch == \"resnext\":\n",
    "        args.model_cardinality = 8\n",
    "        args.model_depth = 29\n",
    "        args.model_width = 64\n",
    "\n",
    "elif args.dataset == \"pss\":\n",
    "    args.num_classes = 3\n",
    "    if args.arch == \"wideresnet\":\n",
    "        args.model_depth = 28\n",
    "        args.model_width = 2\n",
    "    elif args.arch == \"resnext\":\n",
    "        args.model_cardinality = 4\n",
    "        args.model_depth = 28\n",
    "        args.model_width = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from randaugment import RandAugmentMC\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "pss_mean = (0.52661989, 0.42101473, 0.34587943)\n",
    "pss_std = (0.28316404, 0.2779676, 0.28667751)\n",
    "\n",
    "\n",
    "class TransformFixMatch(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.weak = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((32, 32)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomCrop(\n",
    "                    size=32,\n",
    "                    padding=int(32 * 0.125),\n",
    "                    padding_mode=\"reflect\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.strong = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((32, 32)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomCrop(\n",
    "                    size=32, padding=int(32 * 0.125), padding_mode=\"reflect\"\n",
    "                ),\n",
    "                RandAugmentMC(n=2, m=10),\n",
    "            ]\n",
    "        )\n",
    "        self.normalize = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)]\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        weak = self.weak(x)\n",
    "        strong = self.strong(x)\n",
    "        return self.normalize(weak), self.normalize(strong)\n",
    "\n",
    "\n",
    "class PSS_Unlabeled(Dataset):\n",
    "    def __init__(self, img_dir, transform):\n",
    "        self.ids = os.listdir(img_dir)\n",
    "        self.ids.sort()\n",
    "\n",
    "        self.images_fps = [os.path.join(img_dir, image_id) for image_id in self.ids]\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_fps)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image = Image.open(self.images_fps[i])\n",
    "        img_transformed = self.transform(image)\n",
    "\n",
    "        return img_transformed\n",
    "        # img_transform = transforms.Compose([transforms.PILToTensor()])\n",
    "\n",
    "        # return img_transform(img_transformed)\n",
    "\n",
    "\n",
    "def x_u_split(args, labels):\n",
    "    label_per_class = args.num_labeled // args.num_classes\n",
    "    labels = np.array(labels)\n",
    "    labeled_idx = []\n",
    "    # unlabeled data: all data (https://github.com/kekmodel/FixMatch-pytorch/issues/10)\n",
    "    unlabeled_idx = np.array(range(len(labels)))\n",
    "    for i in range(args.num_classes):\n",
    "        idx = np.where(labels == i)[0]\n",
    "        idx = np.random.choice(idx, label_per_class, False)\n",
    "        labeled_idx.extend(idx)\n",
    "    labeled_idx = np.array(labeled_idx)\n",
    "    assert len(labeled_idx) == args.num_labeled\n",
    "\n",
    "    if args.expand_labels or args.num_labeled < args.batch_size:\n",
    "        num_expand_x = math.ceil(args.batch_size * args.eval_step / args.num_labeled)\n",
    "        labeled_idx = np.hstack([labeled_idx for _ in range(num_expand_x)])\n",
    "    np.random.shuffle(labeled_idx)\n",
    "    return labeled_idx, unlabeled_idx\n",
    "\n",
    "\n",
    "def get_pss(args, root):\n",
    "    transform_labeled = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(\n",
    "                size=32, padding=int(32 * 0.125), padding_mode=\"reflect\"\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=pss_mean, std=pss_std),\n",
    "        ]\n",
    "    )\n",
    "    transform_val = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=pss_mean, std=pss_std),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_labeled_dataset = datasets.ImageFolder(\n",
    "        root=\"C:\\\\Users\\\\can.michael\\\\Desktop\\\\others\\\\SSL\\\\FixMatch\\\\FixMatch-pytorch\\\\data\\\\pizza_steak_sushi\\\\20label\\\\train\",\n",
    "        transform=transform_labeled,\n",
    "        target_transform=None,\n",
    "    )\n",
    "\n",
    "    train_unlabeled_dataset = PSS_Unlabeled(\n",
    "        img_dir=\"C:\\\\Users\\\\can.michael\\\\Desktop\\\\others\\\\SSL\\\\FixMatch\\\\FixMatch-pytorch\\\\data\\\\pizza_steak_sushi\\\\20label\\\\unlabeled\",\n",
    "        transform=TransformFixMatch(mean=pss_mean, std=pss_std),\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.ImageFolder(\n",
    "        root=\"C:\\\\Users\\\\can.michael\\\\Desktop\\\\others\\\\SSL\\\\FixMatch\\\\FixMatch-pytorch\\\\data\\\\pizza_steak_sushi\\\\20label\\\\test\",\n",
    "        transform=transform_val,\n",
    "        target_transform=None,\n",
    "    )\n",
    "\n",
    "    return train_labeled_dataset, train_unlabeled_dataset, test_dataset\n",
    "\n",
    "labeled_dataset, unlabeled_dataset, test_dataset = get_pss(\n",
    "    args, \"./data\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "train_sampler = RandomSampler if args.local_rank == -1 else DistributedSampler\n",
    "\n",
    "labeled_trainloader = DataLoader(\n",
    "    labeled_dataset,\n",
    "    sampler=train_sampler(labeled_dataset),\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "unlabeled_trainloader = DataLoader(\n",
    "    unlabeled_dataset,\n",
    "    sampler=train_sampler(unlabeled_dataset),\n",
    "    batch_size=args.batch_size * args.mu,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    sampler=SequentialSampler(test_dataset),\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    ")\n",
    "\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/30/2024 15:47:35 - INFO - models.wideresnet -   Model: WideResNet 28x2\n",
      "05/30/2024 15:47:35 - INFO - __main__ -   Total params: 1.47M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WideResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (block1): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model(args):\n",
    "    if args.arch == \"wideresnet\":\n",
    "        import models.wideresnet as models\n",
    "\n",
    "        model = models.build_wideresnet(\n",
    "            depth=args.model_depth,\n",
    "            widen_factor=args.model_width,\n",
    "            dropout=0,\n",
    "            num_classes=args.num_classes,\n",
    "        )\n",
    "    elif args.arch == \"resnext\":\n",
    "        import models.resnext as models\n",
    "\n",
    "        model = models.build_resnext(\n",
    "            cardinality=args.model_cardinality,\n",
    "            depth=args.model_depth,\n",
    "            width=args.model_width,\n",
    "            num_classes=args.num_classes,\n",
    "        )\n",
    "    logger.info(\n",
    "        \"Total params: {:.2f}M\".format(\n",
    "            sum(p.numel() for p in model.parameters()) / 1e6\n",
    "        )\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model(args)\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"bn\"]\n",
    "grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": args.wdecay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = optim.SGD(\n",
    "    grouped_parameters, lr=args.lr, momentum=0.9, nesterov=args.nesterov\n",
    ")\n",
    "\n",
    "args.epochs = math.ceil(args.total_steps / args.eval_step)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, args.warmup, args.total_steps\n",
    ")\n",
    "\n",
    "if args.use_ema:\n",
    "    from models.ema import ModelEMA\n",
    "\n",
    "    ema_model = ModelEMA(args, model, args.ema_decay)\n",
    "\n",
    "args.start_epoch = 0\n",
    "\n",
    "if args.resume:\n",
    "    logger.info(\"==> Resuming from checkpoint..\")\n",
    "    assert os.path.isfile(args.resume), \"Error: no checkpoint directory found!\"\n",
    "    args.out = os.path.dirname(args.resume)\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    best_acc = checkpoint[\"best_acc\"]\n",
    "    args.start_epoch = checkpoint[\"epoch\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    if args.use_ema:\n",
    "        ema_model.ema.load_state_dict(checkpoint[\"ema_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "\n",
    "if args.amp:\n",
    "    from apex import amp\n",
    "\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=args.opt_level)\n",
    "\n",
    "if args.local_rank != -1:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(\n",
    "        model,\n",
    "        device_ids=[args.local_rank],\n",
    "        output_device=args.local_rank,\n",
    "        find_unused_parameters=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, test_loader, model, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    if not args.no_progress:\n",
    "        test_loader = tqdm(test_loader, disable=args.local_rank not in [-1, 0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            data_time.update(time.time() - end)\n",
    "            model.eval()\n",
    "\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device)\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "\n",
    "            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.shape[0])\n",
    "            top1.update(prec1.item(), inputs.shape[0])\n",
    "            top5.update(prec5.item(), inputs.shape[0])\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if not args.no_progress:\n",
    "                test_loader.set_description(\n",
    "                    \"Test Iter: {batch:4}/{iter:4}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. top1: {top1:.2f}. top5: {top5:.2f}. \".format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        iter=len(test_loader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                    )\n",
    "                )\n",
    "        if not args.no_progress:\n",
    "            test_loader.close()\n",
    "\n",
    "    logger.info(\"top-1 acc: {:.2f}\".format(top1.avg))\n",
    "    logger.info(\"top-5 acc: {:.2f}\".format(top5.avg))\n",
    "    return losses.avg, top1.avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    args,\n",
    "    labeled_trainloader,\n",
    "    unlabeled_trainloader,\n",
    "    test_loader,\n",
    "    model,\n",
    "    optimizer,\n",
    "    ema_model,\n",
    "    scheduler,\n",
    "):\n",
    "    if args.amp:\n",
    "        from apex import amp\n",
    "    global best_acc\n",
    "    test_accs = []\n",
    "    end = time.time()\n",
    "\n",
    "    if args.world_size > 1:\n",
    "        labeled_epoch = 0\n",
    "        unlabeled_epoch = 0\n",
    "        labeled_trainloader.sampler.set_epoch(labeled_epoch)\n",
    "        unlabeled_trainloader.sampler.set_epoch(unlabeled_epoch)\n",
    "\n",
    "    labeled_iter = iter(labeled_trainloader)\n",
    "    unlabeled_iter = iter(unlabeled_trainloader)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        losses_x = AverageMeter()\n",
    "        losses_u = AverageMeter()\n",
    "        mask_probs = AverageMeter()\n",
    "        if not args.no_progress:\n",
    "            p_bar = tqdm(range(args.eval_step), disable=args.local_rank not in [-1, 0])\n",
    "        for batch_idx in range(args.eval_step):\n",
    "            try:\n",
    "                inputs_x, targets_x = next(labeled_iter)\n",
    "            except:\n",
    "                if args.world_size > 1:\n",
    "                    labeled_epoch += 1\n",
    "                    labeled_trainloader.sampler.set_epoch(labeled_epoch)\n",
    "                labeled_iter = iter(labeled_trainloader)\n",
    "                inputs_x, targets_x = next(labeled_iter)\n",
    "\n",
    "            try:\n",
    "                unlabeled_pair = next(unlabeled_iter)\n",
    "                (inputs_u_w, inputs_u_s), _ = unlabeled_pair[0], unlabeled_pair[1]\n",
    "            except:\n",
    "                if args.world_size > 1:\n",
    "                    unlabeled_epoch += 1\n",
    "                    unlabeled_trainloader.sampler.set_epoch(unlabeled_epoch)\n",
    "                unlabeled_iter = iter(unlabeled_trainloader)\n",
    "                unlabeled_pair = next(unlabeled_iter)\n",
    "                (inputs_u_w, inputs_u_s), _ = unlabeled_pair[0], unlabeled_pair[1]\n",
    "\n",
    "            data_time.update(time.time() - end)\n",
    "            batch_size = inputs_x.shape[0]\n",
    "            inputs = interleave(\n",
    "                torch.cat((inputs_x, inputs_u_w, inputs_u_s)), 2 * args.mu + 1\n",
    "            ).to(args.device)\n",
    "            targets_x = targets_x.to(args.device)\n",
    "            logits = model(inputs)\n",
    "            logits = de_interleave(logits, 2 * args.mu + 1)\n",
    "            logits_x = logits[:batch_size]\n",
    "            logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
    "            del logits\n",
    "\n",
    "            targets_x = targets_x.long()\n",
    "\n",
    "            Lx = F.cross_entropy(logits_x, targets_x, reduction=\"mean\")\n",
    "\n",
    "            pseudo_label = torch.softmax(logits_u_w.detach() / args.T, dim=-1)\n",
    "            max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
    "            mask = max_probs.ge(args.threshold).float()\n",
    "\n",
    "            Lu = (\n",
    "                F.cross_entropy(logits_u_s, targets_u, reduction=\"none\") * mask\n",
    "            ).mean()\n",
    "\n",
    "            loss = Lx + args.lambda_u * Lu\n",
    "\n",
    "            if args.amp:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            losses.update(loss.item())\n",
    "            losses_x.update(Lx.item())\n",
    "            losses_u.update(Lu.item())\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if args.use_ema:\n",
    "                ema_model.update(model)\n",
    "            model.zero_grad()\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            mask_probs.update(mask.mean().item())\n",
    "            if not args.no_progress:\n",
    "                p_bar.set_description(\n",
    "                    \"Train Epoch: {epoch}/{epochs:4}. Iter: {batch:4}/{iter:4}. LR: {lr:.4f}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. Loss_x: {loss_x:.4f}. Loss_u: {loss_u:.4f}. Mask: {mask:.2f}. \".format(\n",
    "                        epoch=epoch + 1,\n",
    "                        epochs=args.epochs,\n",
    "                        batch=batch_idx + 1,\n",
    "                        iter=args.eval_step,\n",
    "                        lr=scheduler.get_last_lr()[0],\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        loss_x=losses_x.avg,\n",
    "                        loss_u=losses_u.avg,\n",
    "                        mask=mask_probs.avg,\n",
    "                    )\n",
    "                )\n",
    "                p_bar.update()\n",
    "\n",
    "        if not args.no_progress:\n",
    "            p_bar.close()\n",
    "\n",
    "        if args.use_ema:\n",
    "            test_model = ema_model.ema\n",
    "        else:\n",
    "            test_model = model\n",
    "\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            test_loss, test_acc = test(args, test_loader, test_model, epoch)\n",
    "\n",
    "            # args.writer.add_scalar(\"train/1.train_loss\", losses.avg, epoch)\n",
    "            # args.writer.add_scalar(\"train/2.train_loss_x\", losses_x.avg, epoch)\n",
    "            # args.writer.add_scalar(\"train/3.train_loss_u\", losses_u.avg, epoch)\n",
    "            # args.writer.add_scalar(\"train/4.mask\", mask_probs.avg, epoch)\n",
    "            # args.writer.add_scalar(\"test/1.test_acc\", test_acc, epoch)\n",
    "            # args.writer.add_scalar(\"test/2.test_loss\", test_loss, epoch)\n",
    "            # TODO CMH UNCOMMENT BURAYI HIZLANSIN DIYE YORUMA ALDIN\n",
    "\n",
    "            is_best = test_acc > best_acc\n",
    "            best_acc = max(test_acc, best_acc)\n",
    "\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "            if args.use_ema:\n",
    "                ema_to_save = (\n",
    "                    ema_model.ema.module\n",
    "                    if hasattr(ema_model.ema, \"module\")\n",
    "                    else ema_model.ema\n",
    "                )\n",
    "            # save_checkpoint(\n",
    "            #     {\n",
    "            #         \"epoch\": epoch + 1,\n",
    "            #         \"state_dict\": model_to_save.state_dict(),\n",
    "            #         \"ema_state_dict\": (\n",
    "            #             ema_to_save.state_dict() if args.use_ema else None\n",
    "            #         ),\n",
    "            #         \"acc\": test_acc,\n",
    "            #         \"best_acc\": best_acc,\n",
    "            #         \"optimizer\": optimizer.state_dict(),\n",
    "            #         \"scheduler\": scheduler.state_dict(),\n",
    "            #     },\n",
    "            #     is_best,\n",
    "            #     args.out,\n",
    "            # ) TODO CMH UNCOMMENT BURAYI HIZLANSIN DIYE YORUMA ALDIN\n",
    "\n",
    "            test_accs.append(test_acc)\n",
    "            logger.info(\"Best top-1 acc: {:.2f}\".format(best_acc))\n",
    "            logger.info(\"Mean top-1 acc: {:.2f}\\n\".format(np.mean(test_accs[-20:])))\n",
    "\n",
    "    # if args.local_rank in [-1, 0]:\n",
    "    # args.writer.close() TODO CMH UNCOMMENT BURAYI HIZLANSIN DIYE YORUMA ALDIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/30/2024 15:47:44 - INFO - __main__ -   ***** Running training *****\n",
      "05/30/2024 15:47:44 - INFO - __main__ -     Task = cifar10@4000\n",
      "05/30/2024 15:47:44 - INFO - __main__ -     Num Epochs = 1024\n",
      "05/30/2024 15:47:44 - INFO - __main__ -     Batch size per GPU = 8\n",
      "05/30/2024 15:47:44 - INFO - __main__ -     Total train batch size = 8\n",
      "05/30/2024 15:47:44 - INFO - __main__ -     Total optimization steps = 1048576\n",
      "  0%|          | 0/1024 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 8680, 20416, 18584, 18972) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\can.michael\\AppData\\Local\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\can.michael\\AppData\\Local\\anaconda3\\envs\\torchenv\\Lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, labeled_trainloader, unlabeled_trainloader, test_loader, model, optimizer, ema_model, scheduler)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     unlabeled_pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(unlabeled_iter)\n\u001b[0;32m     48\u001b[0m     (inputs_u_w, inputs_u_s), _ \u001b[38;5;241m=\u001b[39m unlabeled_pair[\u001b[38;5;241m0\u001b[39m], unlabeled_pair[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\can.michael\\AppData\\Local\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\can.michael\\AppData\\Local\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\can.michael\\AppData\\Local\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m     success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "File \u001b[1;32mc:\\Users\\can.michael\\AppData\\Local\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 3148, 20180, 8272, 23128) exited unexpectedly",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\can.michael\\AppData\\Local\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\can.michael\\AppData\\Local\\anaconda3\\envs\\torchenv\\Lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Total optimization steps = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mtotal_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 9\u001b[0m train(\n\u001b[0;32m     10\u001b[0m     args,\n\u001b[0;32m     11\u001b[0m     labeled_trainloader,\n\u001b[0;32m     12\u001b[0m     unlabeled_trainloader,\n\u001b[0;32m     13\u001b[0m     test_loader,\n\u001b[0;32m     14\u001b[0m     model,\n\u001b[0;32m     15\u001b[0m     optimizer,\n\u001b[0;32m     16\u001b[0m     ema_model,\n\u001b[0;32m     17\u001b[0m     scheduler,\n\u001b[0;32m     18\u001b[0m )\n",
      "Cell \u001b[1;32mIn[12], line 54\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, labeled_trainloader, unlabeled_trainloader, test_loader, model, optimizer, ema_model, scheduler)\u001b[0m\n\u001b[0;32m     52\u001b[0m         unlabeled_trainloader\u001b[38;5;241m.\u001b[39msampler\u001b[38;5;241m.\u001b[39mset_epoch(unlabeled_epoch)\n\u001b[0;32m     53\u001b[0m     unlabeled_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(unlabeled_trainloader)\n\u001b[1;32m---> 54\u001b[0m     unlabeled_pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(unlabeled_iter)\n\u001b[0;32m     55\u001b[0m     (inputs_u_w, inputs_u_s), _ \u001b[38;5;241m=\u001b[39m unlabeled_pair[\u001b[38;5;241m0\u001b[39m], unlabeled_pair[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     57\u001b[0m data_time\u001b[38;5;241m.\u001b[39mupdate(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m end)\n",
      "File \u001b[1;32mc:\\Users\\can.michael\\AppData\\Local\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\can.michael\\AppData\\Local\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\can.michael\\AppData\\Local\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\can.michael\\AppData\\Local\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 8680, 20416, 18584, 18972) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Task = {args.dataset}@{args.num_labeled}\")\n",
    "logger.info(f\"  Num Epochs = {args.epochs}\")\n",
    "logger.info(f\"  Batch size per GPU = {args.batch_size}\")\n",
    "logger.info(f\"  Total train batch size = {args.batch_size*args.world_size}\")\n",
    "logger.info(f\"  Total optimization steps = {args.total_steps}\")\n",
    "\n",
    "model.zero_grad()\n",
    "train(\n",
    "    args,\n",
    "    labeled_trainloader,\n",
    "    unlabeled_trainloader,\n",
    "    test_loader,\n",
    "    model,\n",
    "    optimizer,\n",
    "    ema_model,\n",
    "    scheduler,\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
